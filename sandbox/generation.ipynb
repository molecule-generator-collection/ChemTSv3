{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for local\n",
        "import sys\n",
        "repo_root = \"../\"\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<frozen importlib._bootstrap>:241: RuntimeWarning: to-Python converter for class boost::shared_ptr<class RDKit::FilterHierarchyMatcher> already registered; second conversion method ignored.\n",
            "c:\\Users\\fsfsf\\anaconda3\\envs\\v3-forge\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\fsfsf\\anaconda3\\envs\\v3-forge\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 指定されたプロシージャが見つかりません。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from datetime import datetime\n",
        "import importlib\n",
        "import numpy as np\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "from typing import Any\n",
        "import yaml\n",
        "from rdkit import RDLogger\n",
        "from generator import Generator\n",
        "from language import Language, MolLanguage\n",
        "from node import SurrogateNode, SentenceNode, MolSentenceNode\n",
        "from utils import add_sep, class_from_package, make_logger\n",
        "\n",
        "RDLogger.DisableLog('rdApp.*') # use debug: true to log invalid mols\n",
        "\n",
        "def read_yaml(yaml_path: str) -> tuple[Generator, dict[str, Any]]:\n",
        "    with open(os.path.join(repo_root, yaml_path)) as f:\n",
        "        conf = yaml.safe_load(f)\n",
        "\n",
        "    output_dir=os.path.join(repo_root, \"sandbox\", conf[\"output_dir\"], datetime.now().strftime(\"%m-%d_%H-%M\")) + os.sep\n",
        "    file_level = logging.DEBUG if conf.get(\"debug\") else logging.INFO\n",
        "    logger = make_logger(output_dir, file_level=file_level)\n",
        "    generator_args = conf.get(\"generator_args\", {})\n",
        "\n",
        "    transition_args = conf.get(\"transition_args\", {})\n",
        "    if \"model_dir\" in transition_args:\n",
        "        transition_args[\"model_dir\"] = os.path.join(repo_root, transition_args[\"model_dir\"])\n",
        "    lang_path = conf.get(\"lang_path\")\n",
        "    if lang_path is None:\n",
        "        lang_name = os.path.basename(os.path.normpath(transition_args[\"model_dir\"])) + \".lang\"\n",
        "        lang_path = add_sep(transition_args[\"model_dir\"]) + lang_name\n",
        "    lang = Language.load(lang_path)\n",
        "    transition_class = class_from_package(\"transition\", conf[\"transition_class\"])\n",
        "    transition = transition_class(lang=lang, logger=logger, device=conf.get(\"device\"), **transition_args)\n",
        "\n",
        "    reward_class = class_from_package(\"reward\", conf.get(\"reward_class\"))\n",
        "    reward = reward_class(**conf.get(\"reward_args\", {}))\n",
        "    \n",
        "    if \"policy_class\" in conf:\n",
        "        policy_class = class_from_package(\"policy\", conf.get(\"policy_class\"))\n",
        "        policy = policy_class(**conf.get(\"policy_args\", {}))\n",
        "        generator_args[\"policy\"] = policy\n",
        "\n",
        "    filter_settings = conf.get(\"filters\", [])\n",
        "    filters = []\n",
        "    for s in filter_settings:\n",
        "        filter_class = class_from_package(\"filter\", s.pop(\"filter_class\"))\n",
        "        filters.append(filter_class(**s))\n",
        "\n",
        "    if \"node_class\" in conf:\n",
        "        node_class = class_from_package(\"node\", conf.get(\"node_class\"))\n",
        "    elif issubclass(lang.__class__, MolLanguage):\n",
        "        node_class = MolSentenceNode\n",
        "    elif issubclass(lang.__class__, Language):\n",
        "        node_class = SentenceNode\n",
        "   \n",
        "    if type(conf.get(\"root\")) == list:\n",
        "        root = SurrogateNode()\n",
        "        for s in conf.get(\"root\"):\n",
        "            node = node_class.node_from_string(lang=lang, string=s, device=conf.get(\"device\"))\n",
        "            root.add_child(action=s, child=node)\n",
        "    else:\n",
        "        root = node_class.node_from_string(lang=lang, string=conf.get(\"root\", \"\"), device=conf.get(\"device\"))\n",
        "    \n",
        "    generator_class = class_from_package(\"generator\", conf.get(\"generator_class\", \"MCTS\"))\n",
        "    generator = generator_class(root=root, transition=transition, reward=reward, filters=filters, output_dir=output_dir, logger=logger, **generator_args)\n",
        "\n",
        "    src = os.path.join(repo_root, yaml_path)\n",
        "    dst = os.path.join(output_dir, \"setting.yaml\")\n",
        "    shutil.copy(src, dst)\n",
        "    return generator, conf\n",
        "\n",
        "def generate_and_analyze(generator, conf):\n",
        "    generator.generate(time_limit=conf.get(\"time_limit\"), max_generations=conf.get(\"max_generations\"))\n",
        "    generator.plot(**conf.get(\"plot_args\", {}))\n",
        "    generator.analyze()\n",
        "\n",
        "def queue_generations(*args: str):\n",
        "    for yaml_path in args:\n",
        "        generator, conf = read_yaml(yaml_path)\n",
        "        generate_and_analyze(generator, conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KsQZ5Dx_zWtD",
        "outputId": "ad4867f1-023b-4634-ae52-fa0f64488692"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Starting generation...\n",
            "<best reward updated> order: 1, time: 0.00, reward: 0.2669, node: C[C@@H](C)CN(CC)C(=O)Nc1c(C)nn(-c2ccccc2)c1Cl\n",
            "<best reward updated> order: 2, time: 0.03, reward: 0.3074, node: NC(=O)[C@H]1CCCN(C(=O)/C=C/c2ccccc2)C1\n",
            "<best reward updated> order: 3, time: 0.09, reward: 0.3147, node: Fc1ccsc1COc1cncc(N2CCO[C@@H](c3ccc(Br)cc3)C2)n1\n",
            "<best reward updated> order: 5, time: 0.16, reward: 0.3242, node: NCNC(=O)C(=O)Nc1cccc(F)c1F\n",
            "<best reward updated> order: 11, time: 0.43, reward: 0.3330, node: N[C@@H]1CCc2c(sc3ccc(F)cc23)C1\n",
            "<best reward updated> order: 22, time: 1.06, reward: 0.3501, node: N[C@@H](Cc1ccc(Br)cc1)Cc1nccn1C(F)F\n",
            "<best reward updated> order: 25, time: 1.26, reward: 0.3871, node: C=Cc1c(C)nn(C[C@@H](O)COc2ccc(F)cc2)c1[N+](=O)[O-]\n",
            "<best reward updated> order: 26, time: 1.40, reward: 0.4426, node: C[C@H](Sc1ccc(C(=O)[O-])cc1)C(=O)NNc1ccc(F)c(F)c1F\n",
            "generated: 100, time: 7.59, average over 100: 0.2010\n",
            "generated: 200, time: 15.04, average over 100: 0.2016\n",
            "generated: 300, time: 22.78, average over 100: 0.2336\n",
            "generated: 400, time: 30.85, average over 100: 0.2210\n",
            "generated: 500, time: 38.27, average over 100: 0.2301\n",
            "generated: 600, time: 46.53, average over 100: 0.2402\n",
            "generated: 700, time: 54.57, average over 100: 0.1918\n",
            "<best reward updated> order: 797, time: 62.37, reward: 0.4744, node: OC(CN1C(=O)CCC1=O)c1ccc(-c2ccncc2)o1\n",
            "generated: 800, time: 62.56, average over 100: 0.2351\n",
            "generated: 900, time: 71.21, average over 100: 0.2518\n",
            "generated: 1000, time: 80.69, average over 100: 0.2636\n",
            "generated: 1100, time: 88.89, average over 100: 0.2760\n",
            "generated: 1200, time: 96.94, average over 100: 0.2458\n",
            "generated: 1300, time: 104.92, average over 100: 0.2703\n",
            "generated: 1400, time: 112.93, average over 100: 0.3366\n",
            "generated: 1500, time: 121.91, average over 100: 0.3022\n",
            "<best reward updated> order: 1501, time: 122.08, reward: 0.4875, node: O[C@H](CSCC1=CCOC1)c1ccc(F)cc1\n",
            "generated: 1600, time: 129.32, average over 100: 0.3183\n",
            "generated: 1700, time: 138.30, average over 100: 0.3227\n",
            "generated: 1800, time: 145.61, average over 100: 0.3293\n"
          ]
        }
      ],
      "source": [
        "# start generation\n",
        "c1 = \"config/mcts_d_score_replication.yaml\"\n",
        "c2 = \"config/mcts_helm.yaml\"\n",
        "c3 = \"config/mcts_smiles.yaml\"\n",
        "c4 = \"config/model_test.yaml\"\n",
        "c5 = \"config/mcts_d_score.yaml\"\n",
        "c6 = \"config/mcts_j_score.yaml\"\n",
        "generator, conf = read_yaml(c5)\n",
        "generate_and_analyze(generator, conf)\n",
        "# queue_generations(c1, c2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#continue search\n",
        "generator.generate(time_limit=conf.get(\"time_limit\") - generator.passed_time, max_generations=conf.get(\"max_generations\") - len(generator.unique_keys))\n",
        "generator.plot(**conf.get(\"plot_args\", {}))\n",
        "generator.analyze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EC6dDxCrF1dA"
      },
      "outputs": [],
      "source": [
        "#save mcts\n",
        "save_path = generator.output_dir() + \"save.mcts\"\n",
        "generator.save(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load mcts test\n",
        "load_path = generator.output_dir() + \"save.mcts\"\n",
        "generator = MCTS.load(load_path, generator.transition) #TODO: separate notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#time test\n",
        "import cProfile\n",
        "import pstats\n",
        "\n",
        "profiler = cProfile.Profile()\n",
        "profiler.enable()\n",
        "read_yaml(c2)\n",
        "profiler.disable()\n",
        "\n",
        "stats = pstats.Stats(profiler)\n",
        "stats.sort_stats(\"cumtime\").print_stats(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([7.738178695018596,\n",
              "  5.380680492166317,\n",
              "  4.343190079318335,\n",
              "  4.663313149682271,\n",
              "  5.224851127291654,\n",
              "  4.654329008014571,\n",
              "  4.843767364097564,\n",
              "  4.935388081712041,\n",
              "  5.2351610291927955,\n",
              "  -2.3846970086718144,\n",
              "  0.991810044601756,\n",
              "  63.55878775057214,\n",
              "  2.521272632444819,\n",
              "  3.9831675518469654,\n",
              "  0.6139719804089897,\n",
              "  303.283,\n",
              "  1,\n",
              "  1],\n",
              " 0.6487755981901583)"
            ]
          },
          "execution_count": 222,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#DScore test\n",
        "from reward import DScoreReward\n",
        "from node import MolSentenceNode\n",
        "cf = \"config/mcts_d_score.yaml\"\n",
        "generator, conf = read_yaml(cf)\n",
        "string = \"c1sc2ncncc2c1[C@@H](O)c1=nc(C([O-])=O)=nc(N)=n1\"\n",
        "x = MolSentenceNode.node_from_string(lang=generator.transition.lang, string=string, include_eos=True, device=\"cpu\")\n",
        "generator.reward.objective_values_and_reward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c1sc2ncncc2c1- 0.055533841252326965\n",
            "c1sc2ncncc2c1/ 0.028348693624138832\n",
            "c1sc2ncncc2c1 0.06410867720842361\n",
            "c1sc2ncncc2c1C 0.4694652259349823\n",
            "c1sc2ncncc2c1F 0.0009618543554097414\n",
            "c1sc2ncncc2c1I 0.0021593605633825064\n",
            "c1sc2ncncc2c1N 0.19565324485301971\n",
            "c1sc2ncncc2c1O 0.017446408048272133\n",
            "c1sc2ncncc2c1S 0.007149592507630587\n",
            "c1sc2ncncc2c1[C@@H] 0.06944021582603455\n",
            "c1sc2ncncc2c1[C@@] 0.002234969986602664\n",
            "c1sc2ncncc2c1[C@H] 0.05132880061864853\n",
            "c1sc2ncncc2c1[C@] 0.0014879098162055016\n",
            "c1sc2ncncc2c1[N-] 0.004273803438991308\n",
            "c1sc2ncncc2c1[O-] 0.012141085229814053\n",
            "c1sc2ncncc2c1c 0.013975519686937332\n",
            "c1sc2ncncc2c1n 0.004290837794542313\n"
          ]
        }
      ],
      "source": [
        "string = \"c1sc2ncncc2c1\"\n",
        "y = MolSentenceNode.node_from_string(lang=generator.transition.lang, string=string, include_eos=False, device=\"cpu\")\n",
        "t = generator.transition.transitions_with_probs(y)\n",
        "for action, node, prob in t:\n",
        "    print(node, prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([11.004699999999985, 2.7979856545812734, 0], 0.8645154119818951)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#JScore test\n",
        "from reward import DScoreReward\n",
        "from node import MolSentenceNode\n",
        "cf = \"config/mcts_j_score.yaml\"\n",
        "generator, conf = read_yaml(cf)\n",
        "string = \"O=C(Nc1cc(Nc2c(Cl)cccc2NCc2ccc(Cl)cc2Cl)ccc1C1=CCCCC1)c1cc(F)cc(Cl)c1\"\n",
        "x = MolSentenceNode.node_from_string(lang=generator.transition.lang, string=string, include_eos=True, device=\"cpu\")\n",
        "generator.reward.objective_values_and_reward(x)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "v3-forge",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
